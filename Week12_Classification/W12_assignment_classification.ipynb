{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning assignment: a bigger classification of basalt source\n",
    "\n",
    "In the 2006 paper\n",
    "\n",
    ">Vermeesch, P. (2006). Tectonic discrimination of basalts with classification trees. Geochimica et Cosmochimica Acta, 70, 1839-1848. https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "Vermeesch wrote:\n",
    "\n",
    "> *\"If a much larger database were compiled, the trees would grow and their discriminative power increase, but they would still be easy to interpret\"*\n",
    "\n",
    "In a more recent paper, Doucet et al. compiled many more data. Rather than 756 basalt data points, they compiled 29,407 of which 22,005 correspond to the categories of Vermeesch (2006).\n",
    "\n",
    "> Doucet, L. S., Tetley, M. G., Li, Z.-X., Liu, Y., & Gamaleldien, H. (2022). Geochemical fingerprinting of continental and oceanic basalts: A machine learning approach. Earth-Science Reviews, 233, https://doi.org/10.1016/j.earscirev.2022.104192\n",
    "\n",
    "Your task in this assignment is use the data of Doucet et al. (2022) to evaluate whether the predictive power of the classification tree approach increases within this increase in data size as predicted by Vermeesch (2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import scientific Python libraries\n",
    "\n",
    "In addition to the standard scientific Python libraries, a number of functions from `sklearn` with be needed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "We will import the data from Doucet et al. 2022 that is provided as their supplemental table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doucet_data = pd.read_csv('./data/Doucet2022.csv',header=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doucet et al. 2022 study includes data from additional basalt types. To test Vermeesch's hypothesis, let's filter the data to be those from:\n",
    "\n",
    "- ***Island arc basalts (IAB)*** *In the Doucet et al. dataset these are called `ARC-O` standing for oceanic arc.*\n",
    "- ***Mid-ocean ridge (MORB)***\n",
    "- ***Ocean-island (OIB)***\n",
    "\n",
    "The code below filters to these types and creates a new dataframe called `basalt_data_MORB_OIB_IAB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data = Doucet_data[(Doucet_data['type']=='MORB') | (Doucet_data['type']=='OIB') | (Doucet_data['type']=='ARC-O')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a decision tree classifier\n",
    "\n",
    "Take the same approach that we did in class to build a decision tree classifier between the different `type` values (as they are called in the Doucet et al. (2022) data set. You will want to take this steps:\n",
    "\n",
    "- Encode the target variable 'type' using LabelEncoder\n",
    "- Split the data into features (X) and target (y)\n",
    "    - When you do this split go ahead and drop the `['type','location','X1']` from X as we don't want them to be part of the classification. You can drop them with this code: \n",
    "    > `X = basalt_data.drop(['type','location','X1'], axis=1)`\n",
    "- Impute missing values using median imputation\n",
    "- Split the data into training and testing sets\n",
    "- Train the decision tree classifier\n",
    "- Make predictions on the test set\n",
    "- Evaluate the classifier\n",
    "- Plot the tree\n",
    "- Get and disply the feature importances from the classifier\n",
    "\n",
    "### Setting the `max_depth`\n",
    "One consideration is that when setting the classifier there is a parameter `max_depth` than can be set to constrain the maximum depth of the tree. The default setting is `max_depth=None` which means it will keep going and going until the leafs of the tree contain a single category. For interpretability, it could be beneficial to set a `max_depth` value like so:\n",
    "\n",
    "```\n",
    "classifier = DecisionTreeClassifier(max_depth=12)\n",
    "```\n",
    "\n",
    "Once you have your machine learning classifier working, experiment with the tradeoff of predictive accuracy that comes with decreasing the depth of the tree and try to find a balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the accuracy of the decision tree based on larger dataset from Doucet et al. (2022) compare to that using the smaller dataset from Vermeesch (2006)?**\n",
    "\n",
    "*Write your answer here*\n",
    "\n",
    "**What `max_depth` value do you think represents a good balance between predictive power and model complexity?**\n",
    "\n",
    "*Write your answer here*\n",
    "\n",
    "**What similarities and differences are there between the importance of different data fields (feature importance) between the decision tree built on the Vermeesch (2006) data compilation vs that built on the Doucet et al. (2022) data compilation?**\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extensions\n",
    "\n",
    "- implement a decision tree on Doucet data with all of the categories included\n",
    "- implement other supervised machine learning algorithms on the `basalt_data`. Recall that some require normalization.\n",
    "- import the Vermeesch (2006) dataset and see how well the decision tree based on Doucet et al. (2022) does in terms of its classification. This will entail making sure that the column names and classification names are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
