{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning: classification of basalt source\n",
    "\n",
    "## Import scientific python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "Text from: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "\n",
    "Learning problems fall into a few categories:\n",
    "- **supervised learning**, in which the data comes with additional attributes that we want to predict (Click here to go to the scikit-learn supervised learning page).This problem can be either:\n",
    "    - *classification*: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n",
    "    - *regression*: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n",
    "\n",
    "- **unsupervised learning**, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (Click here to go to the Scikit-Learn unsupervised learning page).\n",
    "\n",
    "### Training set and testing set\n",
    "\n",
    "Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the training set, on which we learn some properties; we call the other set the testing set, on which we test the learned properties.\n",
    "\n",
    "**Today we will focus on classification through a supervised learning approach**\n",
    "\n",
    "*Systems doing this type of analysis are all around us. Consider a spam filter for example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying volcanic rocks\n",
    "\n",
    "<img src=\"./images/volcanic-tectonics.png\" width = 600 align = 'center'>\n",
    "\n",
    "Today we are going to continue dealing with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explore two weeks ago. During class today, we will continue to focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).\n",
    "\n",
    "> Igneous rocks form in a wide variety of tectonic settings,\n",
    "including mid-ocean ridges, ocean islands, and volcanic\n",
    "arcs. It is a problem of great interest to igneous petrologists\n",
    "to recover the original tectonic setting of mafic rocks of the\n",
    "past. When the geological setting alone cannot unambiguously\n",
    "resolve this question, the chemical composition of\n",
    "these rocks might contain the answer. The major, minor,\n",
    "and trace elemental composition of basalts shows large\n",
    "variations, for example as a function of formation depth\n",
    "(e.g., Kushiro and Kuno, 1963) --- *Vermeesch (2006)*\n",
    "\n",
    "For this analysis we are going to use a dataset that was compiled in \n",
    "\n",
    "Vermeesch (2006) Tectonic discrimination of basalts with classification trees, *Geochimica et Cosmochimica Acta*  https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "These data were grouped into 3 categories:\n",
    "\n",
    "- 256 ***Island arc basalts (IAB)*** from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.\n",
    "- 241 ***Mid-ocean ridge (MORB)*** samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.\n",
    "- 259 ***Ocean-island (OIB)*** samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.\n",
    "\n",
    "**Let's look at the illustration above and determine where each of these settings are within a plate tectonic context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "\n",
    "The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data = pd.read_csv('./data/Vermeesch2006.csv')\n",
    "basalt_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basalt_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can geochemical data be used to classify the tectonic setting?\n",
    "\n",
    "These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?\n",
    "\n",
    "A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. *For the moment you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.*\n",
    "\n",
    "Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas *Earth and Planetary Science Letters* https://doi.org/10.1016/0012-821X(82)90120-0\n",
    "\n",
    "### Plot TiO2 (wt%) vs V (ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot colored by 'affinity'\n",
    "sns.scatterplot(data=basalt_data, x='TiO2_wt_percent', y='V_ppm', hue='affinity', edgecolor='k', s=50)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title=\"Affinity\")\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the pandas groupby function to group by affinity and describe the values of one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basalt_data.groupby('affinity')['TiO2_wt_percent'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**CODE FOR YOU TO WRITE: Use the groupby command and describe the grouped vanadium concentration for the data.**\n",
    "\n",
    "*Can we differentiate between the different affinities on titanium or vanadium concentration alone?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye test classification method\n",
    "\n",
    "In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.\n",
    "\n",
    "The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.\n",
    "\n",
    "Let's say that we have three points where their affinity is unknown.\n",
    "- point 1 has TiO2 of 4% and V concentration of 300 ppm\n",
    "- point 2 has TiO2 of 1% and V concentration of 350 ppm\n",
    "- point 3 has TiO2 of 1.9% and V concentration of 200 ppm\n",
    "\n",
    "**Let's take votes on how they should be classified**\n",
    "\n",
    "***WRITE HOW YOU THINK THEY SHOULD BE CLASSIFIED HERE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_1_TiO2 = 4\n",
    "point_1_V = 300\n",
    "point_2_TiO2 = 1\n",
    "point_2_V = 350\n",
    "point_3_TiO2 = 1.9\n",
    "point_3_V = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot colored by 'affinity'\n",
    "sns.scatterplot(data=basalt_data, x='TiO2_wt_percent', y='V_ppm', hue='affinity', edgecolor='k', s=50)\n",
    "\n",
    "# Plot the unknown points\n",
    "plt.scatter(point_1_TiO2,point_1_V,label='unknown point 1',color='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2,point_2_V,label='unknown point 2',color='red',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2,point_3_V,label='unknown point 3',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear classification\n",
    "\n",
    "An approach that has been taken in volcanic geochemistry is to draw lines to use for classification.\n",
    "\n",
    "We are use the package scikit-learn in order to implement such a classification. Scikit-learn is a widely-used Python library for machine learning and data analysis. It provides a wide range of tools for data preprocessing, model selection, and evaluation. Its user-friendly interface and extensive documentation make it a popular choice for researchers, data analysts, and machine learning practitioners.\n",
    "\n",
    "![scikit-learn logo](https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)\n",
    "\n",
    "We can use a tool in scikit-learn called SVC (Support Vector Classification) in order to do such a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import sci-kit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our classifier\n",
    "\n",
    "We can use `SVC(kernel='linear')` as a classifier. The algorithm finds the best straight line, also called a hyperplane, to separate different groups of data. \n",
    "\n",
    "Once the lines have been found they can be used predict the group of new data points based on which side of the line they fall on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svc_linear = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for classification \n",
    "\n",
    "We need to do a bit of prep work on the data first as not all of the data have Ti (wt %) and V (ppm) data. Let's define a new dataframe `basalt_data_Ti_V` that has the rows that contain both values. This will result in us using fewer data than is in the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) & (~basalt_data['V_ppm'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of basalt data:')\n",
    "print(len(basalt_data))\n",
    "print('number of basalt data with both Ti and V:')\n",
    "print(len(basalt_data_Ti_V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning literature and code conventions, uppercase \"X\" is often used to represent the matrix of feature variables (predictors), while lowercase \"y\" is used to represent the target variable (response).\n",
    "\n",
    "This notation is used to visually distinguish between the two variables and indicate that \"X\" is a matrix (usually with multiple columns for each feature), while \"y\" is a vector (usually with a single column representing the target variable). The uppercase \"X\" signifies a multi-dimensional data structure, and the lowercase \"y\" signifies a one-dimensional data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the necessary features and target variable\n",
    "X = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']]\n",
    "y = basalt_data_Ti_V['affinity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables that represent different categories that we have here are: 'MORB', 'IAB', and 'OIB'. However, most machine learning algorithms require numerical inputs. Label encoding is a technique that transforms the categorical variables into numerical labels. We can use the `sklearn.preprocessing` `LabelEncoder` function to do this task for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original 'affinity' categories were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they have now been transformed to an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train the classifier\n",
    "\n",
    "Now that we have `X` as DataFrame of `['TiO2_wt_percent', 'V_ppm']` and `y_encoded` as numerical representation of the categories we can fit the classifier to the data.\n",
    "\n",
    "To do this, we feed the DataFrame of the data and the array of the classification into a `.fit` function preformed on the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svc_linear.fit(X, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundaries\n",
    "\n",
    "Let's make a 101 x 101 grid of x and y values between 0 and the maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of points over the feature space\n",
    "xx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),\n",
    "                     np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xx, yy, s=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the grid\n",
    "\n",
    "We can then predict the class labels for each point in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_svc_linear.predict(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot up those grid with the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predicted class labels to match the shape of the input grid\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "\n",
    "# Plot the decision boundary and the original data points with their labels\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "\n",
    "sns.scatterplot(data=basalt_data, x='TiO2_wt_percent', y='V_ppm', hue='affinity', edgecolor='k', s=50)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the unknown points onto this classified grid and see what their assignment would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predicted class labels to match the shape of the input grid\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "\n",
    "# Plot the decision boundary and the original data points with their labels\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "\n",
    "sns.scatterplot(data=basalt_data, x='TiO2_wt_percent', y='V_ppm', hue='affinity', edgecolor='k', s=50)\n",
    "# Plot the unknown points\n",
    "plt.scatter(point_1_TiO2,point_1_V,label='unknown point 1',color='black',marker='d',s=100)\n",
    "plt.scatter(point_2_TiO2,point_2_V,label='unknown point 2',color='red',marker='>',s=100)\n",
    "plt.scatter(point_3_TiO2,point_3_V,label='unknown point 3',color='yellow',edgecolors='black',marker='s',s=100)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can visually see where the points fall, we can also ask the classifier to predict the values of these unknown points using `classifier_svc_linear.predict()`. We can return the actual labels of the data (rather than the encoded numbers) by using `le.inverse_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_points_encoded = classifier_svc_linear.predict([[point_1_TiO2,point_1_V],\n",
    "                             [point_2_TiO2,point_2_V],\n",
    "                             [point_3_TiO2,point_3_V]])\n",
    "classified_points = le.inverse_transform(classified_points_encoded)\n",
    "classified_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "How good is our linear SVC classifier? To answer this we'll need to find out how frequently our classifications are correct.\n",
    "\n",
    "**Discussion question**\n",
    "\n",
    "*How should be determine the accuracy of this classification scheme using the data that we already have?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import more `sklearn` tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svc_linear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the testing data\n",
    "\n",
    "The test set was held back from training. We can use to to evaluate the model. How often are the categorizations correction? To do this we can predict the categories using `classifier_svc_linear.predict()` and the compare those to the actual labels using `accuracy_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = classifier_svc_linear.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `accuracy_score` computes the proportion of correct predictions out of the total number of predictions. The accuracy score ranges from 0 to 1, where a higher score indicates better classification performance.\n",
    "\n",
    "We can make a plot that is the classification based on the training data and can then plot the test data. The fraction of points that are plotting within the correct classification corresponds to the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_svc_linear.predict(grid)\n",
    "\n",
    "# Reshape the predicted class labels to match the shape of the input grid\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "\n",
    "# Plot the decision boundary and the original data points with their labels\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "\n",
    "plt.scatter(X_test['TiO2_wt_percent'],X_test['V_ppm'],c=y_test,cmap=cmap)\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a decision tree approach\n",
    "\n",
    "While there is a nice simplicity to the linear SVC classifier approach comparing the TiO$_2$ vs V data, we have a lot more information from other aspects of the geochemistry. We might as well use that information as well.\n",
    "\n",
    "Let's use all the data we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a **Decision Trees** approach which is another supervised machine learning algorithm for classification \n",
    "\n",
    "**Decision Trees** are a type of flowchart-like structure where internal nodes represent decisions based on the input features, branches represent the outcome of these decisions, and leaf nodes represent the final output or class label. The primary goal of a decision tree is to recursively split the data into subsets based on feature values that maximize the separation between the classes.\n",
    "\n",
    "*Why use Decision Trees?*\n",
    "\n",
    "- Easy to understand and interpret: Decision Trees are human-readable and can be visualized, making them easy to understand and interpret even for those with limited machine learning experience.\n",
    "- Minimal data preprocessing: Decision Trees do not require extensive data preprocessing, such as scaling or normalization, as they can handle both numerical and categorical features.\n",
    "- Non-linear relationships: Decision Trees can model complex, non-linear relationships between features and target variables.\n",
    "- Feature importance: Decision Trees can provide insights into feature importance, helping to identify the most relevant features for the problem at hand.\n",
    "\n",
    "### Preparing the data for the decision tree\n",
    "\n",
    "1. Encode the target variable 'affinity' using LabelEncoder: The target variable 'affinity' contains categorical data, which needs to be encoded as numerical values for the decision tree classifier. The LabelEncoder from scikit-learn is used to transform the 'affinity' column into numerical labels.\n",
    "\n",
    "2. Split the data into features (X) and target (y): The dataset is split into two parts, features (X) and the target variable (y). Features are the input variables that the classifier will use to make predictions, and the target variable is the output we want the classifier to predict.\n",
    "\n",
    "3. Impute missing values using median imputation: Decision tree classifiers cannot handle missing values in the input data. Therefore, missing values in the dataset need to be imputed (filled in) before training the classifier. Let's us median imputation which replaces the missing values with the median of the non-missing values in the same column. We can import and use the `SimpleImputer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Encode the target variable 'affinity' using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "basalt_data['affinity'] = le.fit_transform(basalt_data['affinity'])\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = basalt_data.drop('affinity', axis=1)\n",
    "y = basalt_data['affinity']\n",
    "\n",
    "# Impute missing values using median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the `DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the decision tree classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Set up the figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "\n",
    "# Get the original class names\n",
    "class_names = le.inverse_transform(np.unique(y)).astype(str)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plot_tree(classifier, filled=True, feature_names=X.columns, class_names=class_names, ax=ax)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('decision_tree.pdf', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What aspects of the data are important for the classification?\n",
    "\n",
    "We want to be able to readily determine what data fields are the most important for the decision tree. We can do that by determining \"feature importance.\" The code below extracts and displays the importance score, also known as Gini importance or Mean Decrease Impurity, which is a measure of how much a feature contributes to the decision-making process of the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the classifier\n",
    "importances = classifier.feature_importances_\n",
    "\n",
    "# Pair the feature names with their corresponding importances\n",
    "feature_importances = list(zip(X.columns, importances))\n",
    "\n",
    "# Create a DataFrame from the feature importances\n",
    "df_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Reset the index and drop the old index column\n",
    "df_feature_importances.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sorted feature importances\n",
    "display(df_feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion question**\n",
    "\n",
    "*If we were going to build a classifier on just two variables, which should be pick?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the classification using a \"confusion matrix\"\n",
    "\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It visually displays the accuracy of a classifier by comparing its predicted labels against the true labels. The matrix consists of rows and columns that represent the true and predicted classes, respectively. Each cell in the matrix corresponds to the number of samples for a specific combination of true and predicted class labels.\n",
    "\n",
    "The main diagonal of the matrix represents the correctly classified instances, while the off-diagonal elements represent the misclassified instances. By analyzing the confusion matrix, you can gain insights into the performance of the classifier and identify where it gets \"confused.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using ConfusionMatrixDisplay\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring other classification algorithms\n",
    "\n",
    "If you go to the scikit-learn homepage you will find many available classifiers: https://scikit-learn.org/stable/index.html. They are nicely illustrated in this code from the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "\n",
    "The decision tree suggested that Ti and Sr were the biggest differentiators. Let's have a look at Ti (wt %) vs Sr (ppm) and apply different classifying algorithms. For many of these algorithms, it is essential to normalize the data. For example, the nearest neighbor is a distance. Consider that in the TiO2 (wt%) vs. Sr (ppm) or V (ppm) the y-axis and x-axis are so different (in part because of different units). So we need to normalize the data.\n",
    "\n",
    "We can use the `sklearn.preprocessing` function `StandardScaler` to help us here.\n",
    "\n",
    "Let's make a `basalt_data_Ti_Sr` dataframe and then apply the `StandardScaler` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data_Ti_Sr = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) & (~basalt_data['Sr_ppm'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract the necessary features and target variable\n",
    "X = basalt_data_Ti_Sr[['TiO2_wt_percent', 'Sr_ppm']]\n",
    "y = basalt_data_Ti_Sr['affinity']\n",
    "\n",
    "# Encode the target variable 'affinity' using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply different classifiers\n",
    "\n",
    "Now that we have normalized the data, we can apply classifiers, I have put in the `KNeighborsClassifier(3)`. **What are the strengths and weaknesses of this classifier?**\n",
    "\n",
    "*write your answer here*\n",
    "\n",
    "Play around with switching the classifier to other ones from the example above. **What would the best classifier to use?**\n",
    "\n",
    "*write your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with changing KNeighborsClassifier(3) to different classifiers\n",
    "classifier = KNeighborsClassifier(3)\n",
    "classifier.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_normalized)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Create a meshgrid \n",
    "h = 0.02  # Mesh grid step size\n",
    "x_min, x_max = X_test_normalized[:, 0].min() - 1, X_test_normalized[:, 0].max() + 1\n",
    "y_min, y_max = X_test_normalized[:, 1].min() - 1, X_test_normalized[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Classify the grid points using the classifier\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_classes = classifier.predict(grid)\n",
    "\n",
    "# Reshape the predicted class labels to match the shape of the input grid\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary and the test data points\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "plt.scatter(X_test_normalized[:, 0], X_test_normalized[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)\n",
    "\n",
    "# Add a legend\n",
    "cbar = plt.colorbar(ticks=[0.375, 1., 1.625])\n",
    "cbar.set_ticklabels(le.inverse_transform([0, 1, 2]))\n",
    "\n",
    "plt.xlabel('Normalized TiO2_wt_percent')\n",
    "plt.ylabel('Normalized Sr_ppm')\n",
    "plt.title('Classifier (Test Data with Decision Boundary)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word of warning\n",
    "\n",
    "As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). There are other types of contextual data that can give insight. For example, Shervais (1982) notes that: \n",
    "> \"More specific evaluation of the tectonic setting of these and other ophiolites requires\n",
    "application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however,\n",
    "is a potentially powerful adjunct to these techniques.\"\n",
    "\n",
    "Additionally, we would like to be able to assign physical processes to any classification given that we are seeking insight into how the Earth works.\n",
    "\n",
    "Ver\n",
    "> \"no classification method based solely on geochemical\n",
    "data will ever be able to perfectly determine the\n",
    "tectonic affinity of basaltic rocks (or other rocks for that\n",
    "matter) simply because there is a lot of actual overlap between\n",
    "the geochemistry of the different tectonic settings.\n",
    "Notably IABs have a much wider range of compositions\n",
    "than either MORBs or OIBs. Therefore, geochemical classification\n",
    "should never be the only basis for determining\n",
    "tectonic affinity. This is especially the case for rocks that\n",
    "have undergone alteration. In such cases, mobile elements\n",
    "such as Sr, which have great discriminative power, cannot\n",
    "be used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
